AWSTemplateFormatVersion: "2010-09-09"
Description: "Cloud Data Processing Pipeline"
Transform: AWS::Serverless-2016-10-31
Parameters:
  DataIngestTopic:
    Type: String
    MinLength: 1
    Default: "reInvent2020/#"
    Description: "MQTT Topic Name that ehs devices will send messages to."

Resources:
  ########################################################################
  # S3
  ########################################################################
  # S3 Buckets
  # Bucket for input data
  IngestDataBucket:
    # DeletionPolicy: Retain
    Type: AWS::S3::Bucket
    Properties:
      LoggingConfiguration:
        DestinationBucketName: !Ref LogsBucket
        LogFilePrefix: iot-data-ingest/
      LifecycleConfiguration:
        Rules:
          - Id: ExpirationRule
            Status: Enabled
            ExpirationInDays: "7"

  # Buckets for processed data
  ProcessedDataBucket:
    # DeletionPolicy: Retain
    Type: AWS::S3::Bucket
    DependsOn:
      - S3NotificationLambdaFunction
      - LogsBucket
    Properties:
      LoggingConfiguration:
        DestinationBucketName: !Ref LogsBucket
        LogFilePrefix: processed-data/
      LifecycleConfiguration:
        Rules:
          - Id: ExpirationRule
            Status: Enabled
            ExpirationInDays: "7"
      NotificationConfiguration: #Update stack with NotificationConfiguration
        LambdaConfigurations:
          - Event: s3:ObjectCreated:Put
            Function: !GetAtt S3NotificationLambdaFunction.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: prefix
                    Value: proccessed-data/

  # Logging bucket
  LogsBucket:
    # DeletionPolicy: Retain
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: LogDeliveryWrite
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W35
            reason: "This is the logs bucket"

  ########################################################################
  # IoT Pipeline
  ########################################################################

  # IoT
  IoTTopicRule:
    Type: AWS::IoT::TopicRule
    Properties:
      TopicRulePayload:
        Description: "Send IoT Device data in raw format to Kinesis Analytics"
        AwsIotSqlVersion: "2016-03-23"
        RuleDisabled: "false"
        Sql: !Sub 'SELECT *
          FROM "${DataIngestTopic}"'
        Actions:
          - Firehose:
              DeliveryStreamName: !Ref IngestDataDeliveryStream
              RoleArn: !Sub "${IoTTopicRuleRole.Arn}"
              Separator: "\n"

  # Ingest Data Delivery Stream
  IngestDataDeliveryStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      S3DestinationConfiguration:
        BucketARN: !GetAtt IngestDataBucket.Arn
        BufferingHints:
          IntervalInSeconds: 300
          SizeInMBs: 100
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref IoTDataProcessingLogGroup
          LogStreamName: "IngestDataS3Delivery"
        CompressionFormat: "UNCOMPRESSED"
        EncryptionConfiguration:
          NoEncryptionConfig: "NoEncryption"
        Prefix: "raw-data/"
        RoleARN: !GetAtt IngestDataDeliveryStreamRole.Arn

  AnomalyDetectionApplication:
    Type: AWS::KinesisAnalytics::Application
    Properties:
      ApplicationCode: CREATE OR REPLACE STREAM "SQL_STREAM_TEMPERATURE" (
        "building_id" VARCHAR(12),
        "temperature" DOUBLE,
        "unit" VARCHAR(12),
        "arrival_time" TIMESTAMP);

        CREATE OR REPLACE PUMP "STREAM_PUMP_TEMPERATURE" AS INSERT INTO "SQL_STREAM_TEMPERATURE"
        SELECT STREAM
        s."building_id" as building_id,
        avg(s."data_value") as temperature,
        s."unit" as unit,
        STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL '1' SECOND) AS "arrival_time"
        FROM "SOURCE_SQL_STREAM_001" s
        WHERE s."data_type" = 'temperature'
        GROUP BY s."building_id", s."unit", STEP(s.ROWTIME BY INTERVAL '1' SECOND), STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL '1' SECOND);

        CREATE OR REPLACE STREAM "TEMPERATURE_ANOMALY_STREAM" (
        "temperature" DOUBLE,
        "arrival_time" TIMESTAMP,
        "ANOMALY_SCORE" DOUBLE,
        "ANOMALY_EXPLANATION" varchar(512));

        CREATE OR REPLACE PUMP "TEMPERATURE_ANOMALY_PUMP" AS
        INSERT INTO "TEMPERATURE_ANOMALY_STREAM"
        SELECT STREAM "temperature", "arrival_time", ANOMALY_SCORE, ANOMALY_EXPLANATION
        FROM TABLE(RANDOM_CUT_FOREST_WITH_EXPLANATION(
        CURSOR(SELECT STREAM * FROM "SQL_STREAM_TEMPERATURE"), 100, 100, 100000, 5, true));

      Inputs:
        - NamePrefix: "SOURCE_SQL_STREAM"
          InputSchema:
            RecordColumns:
              - Name: "data_type"
                SqlType: "VARCHAR(32)"
                Mapping: "$.data_type"
              - Name: "unit"
                SqlType: "VARCHAR(16)"
                Mapping: "$.unit"
              - Name: "data_value"
                SqlType: "DECIMAL"
                Mapping: "$.data_value"
              - Name: "measurement_timestamp"
                SqlType: "BIGINT"
                Mapping: "$.measurement_timestamp"
              - Name: "building_id"
                SqlType: "VARCHAR(32)"
                Mapping: "$.building_id"
              - Name: "sensor_station_id"
                SqlType: "VARCHAR(32)"
                Mapping: "$.sensor_station_id"
            RecordFormat:
              RecordFormatType: "JSON"
              MappingParameters:
                JSONMappingParameters:
                  RecordRowPath: "$"
          KinesisFirehoseInput:
            ResourceARN: !GetAtt IngestDataDeliveryStream.Arn
            RoleARN: !GetAtt KinesisAnalyticsRole.Arn

  AnomalyDetectionApplicationOutput:
    Type: AWS::KinesisAnalytics::ApplicationOutput
    Properties:
      ApplicationName: !Ref AnomalyDetectionApplication
      Output:
        DestinationSchema:
          RecordFormatType: "CSV"
        KinesisFirehoseOutput:
          ResourceARN: !GetAtt ProccessedDataDeliveryStream.Arn
          RoleARN: !GetAtt KinesisAnalyticsRole.Arn
        Name: SQL_STREAM_TEMPERATURE


  # Ingest Data Delivery Stream
  ProccessedDataDeliveryStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      S3DestinationConfiguration:
        BucketARN: !GetAtt ProcessedDataBucket.Arn
        BufferingHints:
          IntervalInSeconds: 300
          SizeInMBs: 100
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref IoTDataProcessingLogGroup
          LogStreamName: "ProccessedDataS3Delivery"
        CompressionFormat: "UNCOMPRESSED"
        EncryptionConfiguration:
          NoEncryptionConfig: "NoEncryption"
        Prefix: "proccessed-data/"
        RoleARN: !GetAtt ProccessedDataDeliveryStreamRole.Arn

  S3NotificationLambdaFunction:
    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction
    Properties:
      CodeUri: lambda_functions/s3_notification_receiver
      Handler: lambda.lambda_handler
      Runtime: python3.7
      Description: Invokes Sagemaker endpoint as new data arrives in S3.
      MemorySize: 256
      Role: !GetAtt LambdaIAMRole.Arn
      Timeout: 300

  ########################################################################
  # IAM Roles & Logs
  ########################################################################
  # Logs
  IoTDataProcessingLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      RetentionInDays: 7

  IoTDataProcessingLogStream:
    Type: AWS::Logs::LogStream
    Properties:
      LogGroupName: !Ref IoTDataProcessingLogGroup

  IngestDataDeliveryStreamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "firehose.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: /
      Policies:
        # Puts objects in IngestDataBucket
        - PolicyName: "IngestDataS3UploadPolicy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:AbortMultipartUpload
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                  - s3:ListBucketMultipartUploads
                Resource:
                  - !Sub "${IngestDataBucket.Arn}"
                  - !Sub "${IngestDataBucket.Arn}/"
                  - !Sub "${IngestDataBucket.Arn}/*"

        # Write to CloudWatch
        - PolicyName: IngestDataDeliveryStreamLogging
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutDestination
                  - logs:PutLogEvents
                Resource: !Join
                  - ""
                  - - "arn:aws:logs:"
                    - !Ref AWS::Region
                    - ":"
                    - !Ref AWS::AccountId
                    - ":log-group:*"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W11
            reason: "The wildcard action in the logs policy is required"

  ProccessedDataDeliveryStreamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "firehose.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: /
      Policies:
        # Puts objects in IngestDataBucket
        - PolicyName: "ProccessedDataS3UploadPolicy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:AbortMultipartUpload
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                  - s3:ListBucketMultipartUploads
                Resource:
                  - !Sub "${ProcessedDataBucket.Arn}"
                  - !Sub "${ProcessedDataBucket.Arn}/"
                  - !Sub "${ProcessedDataBucket.Arn}/*"

        # Write to CloudWatch
        - PolicyName: IngestDataDeliveryStreamLogging
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutDestination
                  - logs:PutLogEvents
                Resource: !Join
                  - ""
                  - - "arn:aws:logs:"
                    - !Ref AWS::Region
                    - ":"
                    - !Ref AWS::AccountId
                    - ":log-group:*"

  IoTTopicRuleRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "iot.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: /
      Policies:
        # Posts to IngestDataDeliveryStream
        - PolicyName: "IoTTopicRulePolicy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              Effect: Allow
              Action:
                - firehose:DescribeDeliveryStream
                - firehose:ListDeliveryStreams
                - firehose:PutRecord
                - firehose:PutRecordBatch
              Resource: !Sub "${IngestDataDeliveryStream.Arn}"

  KinesisAnalyticsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: kinesisanalytics.amazonaws.com
            Action: "sts:AssumeRole"
      Path: "/"
      Policies:
        - PolicyName: Open
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "*"
                Resource: "*"

  LambdaInvokePermission:
    Type: "AWS::Lambda::Permission"
    DependsOn:
      - ProcessedDataBucket
      - S3NotificationLambdaFunction
    Properties:
      FunctionName: !GetAtt S3NotificationLambdaFunction.Arn
      Action: "lambda:InvokeFunction"
      Principal: s3.amazonaws.com
      SourceAccount: !Ref "AWS::AccountId"
      SourceArn: !Sub "arn:aws:s3:::${ProcessedDataBucket}"

  LambdaIAMRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "s3:*"
                  - "sagemaker:*"
                Resource: "*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogGroup"
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: "arn:aws:logs:*:*:*"

Outputs:
  IngestDataDeliveryStreamArn:
    Description: Arn of Ingest Data Delivery Stream
    Value: !GetAtt IngestDataDeliveryStream.Arn
    Export:
      Name: !Sub "${AWS::StackName}-IngestDataDeliveryStreamArn"
  ProcessedDataBucketArn:
    Description: Arn of S3 bucket for processed data
    Value: !GetAtt ProcessedDataBucket.Arn
    Export:
      Name: !Sub "${AWS::StackName}-ProcessedDataBucketArn"
  IoTDataProcessingLogGroup:
    Description: Log Group Name for logging
    Value: !Ref IoTDataProcessingLogGroup
    Export:
      Name: !Sub "${AWS::StackName}-IoTDataProcessingLogGroup"
