AWSTemplateFormatVersion: '2010-09-09'
Description: Cloud Data Processing Pipeline
Transform: AWS::Serverless-2016-10-31
Parameters:
  DataIngestTopic:
    Type: String
    MinLength: 1
    Default: reInvent2020/#
    Description: MQTT Topic Name that ehs devices will send messages to.
Resources:
  IngestDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      LoggingConfiguration:
        DestinationBucketName:
          Ref: LogsBucket
        LogFilePrefix: iot-data-ingest/
      LifecycleConfiguration:
        Rules:
        - Id: ExpirationRule
          Status: Enabled
          ExpirationInDays: '7'
  ProcessedDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      LoggingConfiguration:
        DestinationBucketName:
          Ref: LogsBucket
        LogFilePrefix: processed-data/
      LifecycleConfiguration:
        Rules:
        - Id: ExpirationRule
          Status: Enabled
          ExpirationInDays: '7'
      NotificationConfiguration:
        LambdaConfigurations:
        - Event: s3:ObjectCreated:Put
          Function:
            Fn::GetAtt:
            - S3NotificationLambdaFunction
            - Arn
          Filter:
            S3Key:
              Rules:
              - Name: prefix
                Value: proccessed-data/
  LogsBucket:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: LogDeliveryWrite
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: W35
          reason: This is the logs bucket
  IoTTopicRule:
    Type: AWS::IoT::TopicRule
    Properties:
      TopicRulePayload:
        Description: Send IoT Device data in raw format to Kinesis Analytics
        AwsIotSqlVersion: '2016-03-23'
        RuleDisabled: 'false'
        Sql:
          Fn::Sub: SELECT * FROM "${DataIngestTopic}"
        Actions:
        - Firehose:
            DeliveryStreamName:
              Ref: IngestDataDeliveryStream
            RoleArn:
              Fn::Sub: ${IoTTopicRuleRole.Arn}
            Separator: '

              '
  IngestDataDeliveryStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      S3DestinationConfiguration:
        BucketARN:
          Fn::GetAtt:
          - IngestDataBucket
          - Arn
        BufferingHints:
          IntervalInSeconds: 300
          SizeInMBs: 100
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName:
            Ref: IoTDataProcessingLogGroup
          LogStreamName: IngestDataS3Delivery
        CompressionFormat: UNCOMPRESSED
        EncryptionConfiguration:
          NoEncryptionConfig: NoEncryption
        Prefix: proccessed-data/
        RoleARN:
          Fn::GetAtt:
          - IngestDataDeliveryStreamRole
          - Arn
  AnomalyDetectionApplication:
    Type: AWS::KinesisAnalytics::Application
    Properties:
      ApplicationCode: 'CREATE OR REPLACE STREAM "SQL_STREAM_TEMPERATURE" ( "building_id"
        VARCHAR(12), "temperature" DOUBLE, "unit" VARCHAR(12), "arrival_time" TIMESTAMP);

        CREATE OR REPLACE PUMP "STREAM_PUMP_TEMPERATURE" AS INSERT INTO "SQL_STREAM_TEMPERATURE"
        SELECT STREAM s."building_id" as building_id, avg(s."data_value") as temperature,
        s."unit" as unit, STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL ''1'' SECOND)
        AS "arrival_time" FROM "SOURCE_SQL_STREAM_001" s WHERE s."data_type" = ''temperature''
        GROUP BY s."building_id", s."unit", STEP(s.ROWTIME BY INTERVAL ''1'' SECOND),
        STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL ''1'' SECOND);



        CREATE OR REPLACE STREAM "SQL_STREAM_HUMIDITY" ( "building_id" VARCHAR(12),
        "humidity" DOUBLE, "unit" VARCHAR(12), "arrival_time" TIMESTAMP);

        CREATE OR REPLACE PUMP "STREAM_PUMP_HUMIDITY" AS INSERT INTO "SQL_STREAM_HUMIDITY"
        SELECT STREAM s."building_id" as building_id, avg(s."data_value") as humidity,
        s."unit" as unit, STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL ''1'' SECOND)
        AS "arrival_time" FROM "SOURCE_SQL_STREAM_001" s WHERE s."data_type" = ''humidity''
        GROUP BY s."building_id", s."unit", STEP(s.ROWTIME BY INTERVAL ''1'' SECOND),
        STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL ''1'' SECOND);



        CREATE OR REPLACE STREAM "SQL_STREAM_VIBRATION_FREQ" ( "building_id" VARCHAR(12),
        "vibration_frequency" DOUBLE, "unit" VARCHAR(12), "arrival_time" TIMESTAMP);

        CREATE OR REPLACE PUMP "STREAM_PUMP_VIBRATION_FREQ" AS INSERT INTO "SQL_STREAM_VIBRATION_FREQ"
        SELECT STREAM s."building_id" as building_id, avg(s."data_value") as vibration_frequency,
        s."unit" as unit, STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL ''1'' SECOND)
        AS "arrival_time" FROM "SOURCE_SQL_STREAM_001" s WHERE s."data_type" = ''vibration_frequency''
        GROUP BY s."building_id", s."unit", STEP(s.ROWTIME BY INTERVAL ''1'' SECOND),
        STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL ''1'' SECOND);

        CREATE OR REPLACE STREAM "SQL_STREAM_VIBRATION_DISP" ( "building_id" VARCHAR(12),
        "vibration_displacement" DOUBLE, "unit" VARCHAR(12), "arrival_time" TIMESTAMP);

        CREATE OR REPLACE PUMP "STREAM_PUMP_VIBRATION_DISP" AS INSERT INTO "SQL_STREAM_VIBRATION_DISP"
        SELECT STREAM s."building_id" as building_id, avg(s."data_value") as vibration_displacement,
        s."unit" as unit, STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL ''1'' SECOND)
        AS "arrival_time" FROM "SOURCE_SQL_STREAM_001" s WHERE s."data_type" = ''vibration_displacement''
        GROUP BY s."building_id", s."unit", STEP(s.ROWTIME BY INTERVAL ''1'' SECOND),
        STEP(s.APPROXIMATE_ARRIVAL_TIME BY INTERVAL ''1'' SECOND);



        -- This is the stream joining the two sets of vibration readings CREATE OR
        REPLACE STREAM "SQL_STREAM_VIBRATION_MERGED" ( "building_id" VARCHAR(12),
        "arrival_time" timestamp, "vibration_frequency_value" double, "vibration_frequency_unit"
        VARCHAR(12), "vibration_displacement_value" double, "vibration_displacement_unit"
        VARCHAR(12) );

        CREATE OR REPLACE PUMP "STREAM_PUMP_VIBRATION_MERGED" AS INSERT INTO "SQL_STREAM_VIBRATION_MERGED"
        SELECT STREAM s1."building_id" as "building_id", s1."arrival_time" as "arrival_time",
        s1."vibration_frequency" as "vibration_frequency_value", s1."unit" as "vibration_frequency_unit",
        s2."vibration_displacement" as "vibration_displacement_value", s2."unit" as
        "vibration_displacement_unit" FROM "SQL_STREAM_VIBRATION_FREQ" s1 JOIN "SQL_STREAM_VIBRATION_DISP"
        s2 ON s1."arrival_time"=s2."arrival_time" AND s1."building_id"=s2."building_id";



        CREATE OR REPLACE STREAM "VIBRATION_ANOMALY_STREAM" ( "vibration_frequency_value"
        DOUBLE, "vibration_displacement_value" DOUBLE, "arrival_time" TIMESTAMP, "ANOMALY_SCORE"
        DOUBLE, "ANOMALY_EXPLANATION" varchar(512));

        CREATE OR REPLACE PUMP "VIBRATION_ANOMALY_PUMP" AS INSERT INTO "VIBRATION_ANOMALY_STREAM"
        SELECT STREAM "vibration_frequency_value", "vibration_displacement_value",
        "arrival_time", ANOMALY_SCORE, ANOMALY_EXPLANATION FROM TABLE(RANDOM_CUT_FOREST_WITH_EXPLANATION(
        CURSOR(SELECT STREAM * FROM "SQL_STREAM_VIBRATION_MERGED"), 100, 100, 100000,
        5, true));'
      Inputs:
      - NamePrefix: SOURCE_SQL_STREAM
        InputSchema:
          RecordColumns:
          - Name: data_type
            SqlType: VARCHAR(32)
            Mapping: $.data_type
          - Name: unit
            SqlType: VARCHAR(16)
            Mapping: $.unit
          - Name: data_value
            SqlType: DECIMAL
            Mapping: $.data_value
          - Name: measurement_timestamp
            SqlType: BIGINT
            Mapping: $.measurement_timestamp
          - Name: building_id
            SqlType: VARCHAR(32)
            Mapping: $.building_id
          - Name: sensor_station_id
            SqlType: VARCHAR(32)
            Mapping: $.sensor_station_id
          RecordFormat:
            RecordFormatType: JSON
            MappingParameters:
              JSONMappingParameters:
                RecordRowPath: $
        KinesisFirehoseInput:
          ResourceARN:
            Fn::GetAtt:
            - IngestDataDeliveryStream
            - Arn
          RoleARN:
            Fn::GetAtt:
            - KinesisAnalyticsRole
            - Arn
  AnomalyDetectionApplicationOutput:
    Type: AWS::KinesisAnalytics::ApplicationOutput
    Properties:
      ApplicationName:
        Ref: AnomalyDetectionApplication
      Output:
        DestinationSchema:
          RecordFormatType: CSV
        KinesisFirehoseOutput:
          ResourceARN:
            Fn::GetAtt:
            - ProccessedDataDeliveryStream
            - Arn
          RoleARN:
            Fn::GetAtt:
            - KinesisAnalyticsRole
            - Arn
  ProccessedDataDeliveryStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      S3DestinationConfiguration:
        BucketARN:
          Fn::GetAtt:
          - ProcessedDataBucket
          - Arn
        BufferingHints:
          IntervalInSeconds: 300
          SizeInMBs: 100
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName:
            Ref: IoTDataProcessingLogGroup
          LogStreamName: ProccessedDataS3Delivery
        CompressionFormat: UNCOMPRESSED
        EncryptionConfiguration:
          NoEncryptionConfig: NoEncryption
        Prefix: proccessed-data/
        RoleARN:
          Fn::GetAtt:
          - ProccessedDataDeliveryStreamRole
          - Arn
  S3NotificationLambdaFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: s3://reinvent2020-anomaly-prediction-121984884871-us-west-2-dev/0c5bb32d387e59b2d3d8572f7fbb8876
      Handler: lambda.lambda_handler
      Runtime: python3.7
      Description: Invokes Sagemaker endpoint as new data arrives in S3.
      MemorySize: 256
      Role:
        Fn::GetAtt:
        - LambdaIAMRole
        - Arn
      Timeout: 300
  IoTDataProcessingLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      RetentionInDays: 7
  IoTDataProcessingLogStream:
    Type: AWS::Logs::LogStream
    Properties:
      LogGroupName:
        Ref: IoTDataProcessingLogGroup
  IngestDataDeliveryStreamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - firehose.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: IngestDataS3UploadPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:AbortMultipartUpload
            - s3:GetBucketLocation
            - s3:GetObject
            - s3:PutObject
            - s3:ListBucket
            - s3:ListBucketMultipartUploads
            Resource:
            - Fn::Sub: ${IngestDataBucket.Arn}
            - Fn::Sub: ${IngestDataBucket.Arn}/
            - Fn::Sub: ${IngestDataBucket.Arn}/*
      - PolicyName: IngestDataDeliveryStreamLogging
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutDestination
            - logs:PutLogEvents
            Resource:
              Fn::Join:
              - ''
              - - 'arn:aws:logs:'
                - Ref: AWS::Region
                - ':'
                - Ref: AWS::AccountId
                - :log-group:*
    Metadata:
      cfn_nag:
        rules_to_suppress:
        - id: W11
          reason: The wildcard action in the logs policy is required
  ProccessedDataDeliveryStreamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - firehose.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: ProccessedDataS3UploadPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:AbortMultipartUpload
            - s3:GetBucketLocation
            - s3:GetObject
            - s3:PutObject
            - s3:ListBucket
            - s3:ListBucketMultipartUploads
            Resource:
            - Fn::Sub: ${ProcessedDataBucket.Arn}
            - Fn::Sub: ${ProcessedDataBucket.Arn}/
            - Fn::Sub: ${ProcessedDataBucket.Arn}/*
      - PolicyName: IngestDataDeliveryStreamLogging
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutDestination
            - logs:PutLogEvents
            Resource:
              Fn::Join:
              - ''
              - - 'arn:aws:logs:'
                - Ref: AWS::Region
                - ':'
                - Ref: AWS::AccountId
                - :log-group:*
  IoTTopicRuleRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - iot.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: IoTTopicRulePolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
            Effect: Allow
            Action:
            - firehose:DescribeDeliveryStream
            - firehose:ListDeliveryStreams
            - firehose:PutRecord
            - firehose:PutRecordBatch
            Resource:
              Fn::Sub: ${IngestDataDeliveryStream.Arn}
  KinesisAnalyticsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: kinesisanalytics.amazonaws.com
          Action: sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: Open
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action: '*'
            Resource: '*'
  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName:
        Fn::GetAtt:
        - S3NotificationLambdaFunction
        - Arn
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceAccount:
        Ref: AWS::AccountId
      SourceArn:
        Fn::Sub: arn:aws:s3:::${ProcessedDataBucket}
  LambdaIAMRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: root
        PolicyDocument:
          Version: 2012-10-17
          Statement:
          - Effect: Allow
            Action:
            - s3:*
            - sagemaker:*
            Resource: '*'
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*
Outputs:
  IngestDataDeliveryStreamArn:
    Description: Arn of Ingest Data Delivery Stream
    Value:
      Fn::GetAtt:
      - IngestDataDeliveryStream
      - Arn
    Export:
      Name:
        Fn::Sub: ${AWS::StackName}-IngestDataDeliveryStreamArn
  ProcessedDataBucketArn:
    Description: Arn of S3 bucket for processed data
    Value:
      Fn::GetAtt:
      - ProcessedDataBucket
      - Arn
    Export:
      Name:
        Fn::Sub: ${AWS::StackName}-ProcessedDataBucketArn
  IoTDataProcessingLogGroup:
    Description: Log Group Name for logging
    Value:
      Ref: IoTDataProcessingLogGroup
    Export:
      Name:
        Fn::Sub: ${AWS::StackName}-IoTDataProcessingLogGroup
